{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:07:13.882734Z",
     "start_time": "2024-07-01T02:07:12.954990Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Tongyi(streaming=True, callbacks=[StreamingStdOutCallbackHandler()],temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微观经济学是研究个体经济单位，如家庭、企业等在市场中的行为和决策的学科。它的前提基础主要包括：\n",
      "\n",
      "1. **理性人假设**：微观经济学假定每个经济主体都是理性的，他们会根据自身利益最大化来做出决策。这意味着人们会考虑成本和收益，以达到最佳的经济效果。\n",
      "\n",
      "2. **资源稀缺性假设**：微观经济学认为资源是有限的，但人类的需求是无限的。因此，必须做出选择以最优的方式分配资源。\n",
      "\n",
      "3. **市场机制**：市场经济通过价格机制来调节供求关系，价格的变化可以引导消费者和生产者的决策，使资源得到有效的配置。\n",
      "\n",
      "4. **边际效用递减原理**：这是消费者行为理论的基础，它指出在一定条件下，随着消费量的增加，商品或服务对消费者的边际效用（即每多消费一单位带来的额外满足感）会逐渐减少。\n",
      "\n",
      "5. **完全信息假设**：在理想情况下，微观经济学假设所有市场参与者都拥有完全且准确的信息，以便做出最佳决策。然而，在现实世界中，这个假设往往不成立。\n",
      "这些基本前提为微观经济学的理论构建和分析提供了基础，并帮助我们理解经济主体如何在市场中互动和决策。"
     ]
    },
    {
     "data": {
      "text/plain": "'微观经济学是研究个体经济单位，如家庭、企业等在市场中的行为和决策的学科。它的前提基础主要包括：\\n\\n1. **理性人假设**：微观经济学假定每个经济主体都是理性的，他们会根据自身利益最大化来做出决策。这意味着人们会考虑成本和收益，以达到最佳的经济效果。\\n\\n2. **资源稀缺性假设**：微观经济学认为资源是有限的，但人类的需求是无限的。因此，必须做出选择以最优的方式分配资源。\\n\\n3. **市场机制**：市场经济通过价格机制来调节供求关系，价格的变化可以引导消费者和生产者的决策，使资源得到有效的配置。\\n\\n4. **边际效用递减原理**：这是消费者行为理论的基础，它指出在一定条件下，随着消费量的增加，商品或服务对消费者的边际效用（即每多消费一单位带来的额外满足感）会逐渐减少。\\n\\n5. **完全信息假设**：在理想情况下，微观经济学假设所有市场参与者都拥有完全且准确的信息，以便做出最佳决策。然而，在现实世界中，这个假设往往不成立。\\n\\n这些基本前提为微观经济学的理论构建和分析提供了基础，并帮助我们理解经济主体如何在市场中互动和决策。'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt0=\"请介绍一下微观经济学的前提基础\"\n",
    "llm.invoke(prompt0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:26:37.135296Z",
     "start_time": "2024-06-28T02:26:18.637263Z"
    }
   },
   "id": "5e54a42fbcf54ebc",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m llm\u001B[38;5;241m.\u001B[39mget_num_tokens(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfasfdsafreqwrwe\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\base.py:335\u001B[0m, in \u001B[0;36mBaseLanguageModel.get_num_tokens\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_num_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    325\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get the number of tokens present in the text.\u001B[39;00m\n\u001B[0;32m    326\u001B[0m \n\u001B[0;32m    327\u001B[0m \u001B[38;5;124;03m    Useful for checking if an input will fit in a model's context window.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;124;03m        The integer number of tokens in the text.\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 335\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_token_ids(text))\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\base.py:322\u001B[0m, in \u001B[0;36mBaseLanguageModel.get_token_ids\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcustom_get_token_ids(text)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _get_token_ids_default_method(text)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\base.py:57\u001B[0m, in \u001B[0;36m_get_token_ids_default_method\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Encode the text into token IDs.\"\"\"\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# get the cached tokenizer\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m get_tokenizer()\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mencode(text)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\base.py:51\u001B[0m, in \u001B[0;36mget_tokenizer\u001B[1;34m()\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import transformers python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     47\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is needed in order to calculate get_token_ids. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     48\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install transformers`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     49\u001B[0m     )\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# create a GPT-2 tokenizer instance\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m GPT2TokenizerFast\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2147\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2144\u001B[0m \u001B[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001B[39;00m\n\u001B[0;32m   2145\u001B[0m \u001B[38;5;66;03m# loaded directly from the GGUF file.\u001B[39;00m\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(full_file_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m full_file_name \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m gguf_file:\n\u001B[1;32m-> 2147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m   2148\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load tokenizer for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. If you were trying to load it from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2149\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, make sure you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have a local directory with the same name. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2150\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOtherwise, make sure \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is the correct path to a directory \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2151\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontaining all relevant files for a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokenizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2152\u001B[0m     )\n\u001B[0;32m   2154\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_id, file_path \u001B[38;5;129;01min\u001B[39;00m vocab_files\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   2155\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m file_id \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files:\n",
      "\u001B[1;31mOSError\u001B[0m: Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "llm.get_num_tokens(\"fasfdsafreqwrwe\") # 需要依赖torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:55:42.131747Z",
     "start_time": "2024-06-28T02:54:27.951655Z"
    }
   },
   "id": "dc53418349c56711",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tongyi in module langchain_community.llms.tongyi object:\n",
      "\n",
      "class Tongyi(langchain_core.language_models.llms.BaseLLM)\n",
      " |  Tongyi(*, name: Optional[str] = None, cache: ForwardRef('Union[BaseCache, bool, None]') = None, verbose: bool = None, callbacks: ForwardRef('Callbacks') = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, custom_get_token_ids: Optional[Callable[[str], List[int]]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, client: Any = None, model_name: str = 'qwen-plus', model_kwargs: Dict[str, Any] = None, top_p: float = 0.8, dashscope_api_key: Optional[str] = None, streaming: bool = False, max_retries: int = 10) -> None\n",
      " |  \n",
      " |  Tongyi Qwen large language models.\n",
      " |  \n",
      " |  To use, you should have the ``dashscope`` python package installed, and the\n",
      " |  environment variable ``DASHSCOPE_API_KEY`` set with your API key, or pass\n",
      " |  it as a named parameter to the constructor.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_community.llms import Tongyi\n",
      " |          tongyi = tongyi()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tongyi\n",
      " |      langchain_core.language_models.llms.BaseLLM\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'client': 'Any', 'dashscope_api_key': 'Optional[str...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain_community.llms.tongyi.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'cu...\n",
      " |  \n",
      " |  __fields__ = {'cache': ModelField(name='cache', type=Union[BaseCache, ...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function Tongyi.validate_environm...\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function BaseLLM.raise_deprecation>]\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...ng:...\n",
      " |  \n",
      " |  __validators__ = {'verbose': [<pydantic.class_validators.Validator obj...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] Check Cache and run the LLM on the given prompt and input.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Get a string representation of the object for printing.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, run_id: 'Optional[Union[uuid.UUID, List[Optional[uuid.UUID]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of string prompts.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, run_id: 'Optional[Union[uuid.UUID, List[Optional[uuid.UUID]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of string prompts.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |  \n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      " |      Save the LLM.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to file to save the LLM to.\n",
      " |      \n",
      " |      Example:\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          llm.save(file_path=\"path/llm.yaml\")\n",
      " |  \n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  OutputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  Config = <class 'langchain_core.language_models.llms.BaseLLM.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Get the number of tokens in the messages.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: The message inputs to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sum of the number of tokens across the messages.\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Return the ordered ids of the tokens in a text.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      " |              in the text.\n",
      " |  \n",
      " |  with_structured_output(self, schema: 'Union[Dict, Type[BaseModel]]', **kwargs: 'Any') -> 'Runnable[LanguageModelInput, Union[Dict, BaseModel]]'\n",
      " |      Not implemented on this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.main.ModelMetaclass\n",
      " |      If verbose is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for runnables that can be set at runtime.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-sonnet-20240229\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI()\n",
      " |          )\n",
      " |      \n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |      \n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(\n",
      " |                  configurable={\"llm\": \"openai\"}\n",
      " |              ).invoke(\"which organization created you?\").content\n",
      " |          )\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular runnable fields at runtime.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \",\n",
      " |              model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 200\n",
      " |          print(\"max_tokens_200: \", model.with_config(\n",
      " |              configurable={\"output_token_number\": 200}\n",
      " |              ).invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |  \n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the runnable to JSON.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  get_lc_namespace() -> List[str] from pydantic.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |      \n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  is_lc_serializable() -> bool from pydantic.main.ModelMetaclass\n",
      " |      Is this class serializable?\n",
      " |  \n",
      " |  lc_id() -> List[str] from pydantic.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ainvoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |      \n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |      \n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |      \n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |      \n",
      " |          print(chain_with_assign.input_schema.schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.schema()) #\n",
      " |          {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |  \n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*] Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n",
      " |          generated the event. The root runnable will have an empty list.\n",
      " |          The order of the parent IDs is from the root to the immediate parent.\n",
      " |          Only available for v2 version of the API. The v1 version of the API\n",
      " |          will return an empty list.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      **ATTENTION** This reference table is for the V2 version of the schema.\n",
      " |      \n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      \n",
      " |      Here are declarations associated with the events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      \n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use either `v2` or `v1`.\n",
      " |                   Users should use `v2`.\n",
      " |                   `v1` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run invoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Useful when a runnable in a chain requires an argument that is not\n",
      " |      in the output of the previous runnable or included in the user input.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.chat_models import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |      \n",
      " |          llm = ChatOllama(model='llama2')\n",
      " |      \n",
      " |          # Without bind.\n",
      " |          chain = (\n",
      " |              llm\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |      \n",
      " |          # With bind.\n",
      " |          chain = (\n",
      " |              llm.bind(stop=[\"three\"])\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |  \n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |  \n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |      \n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      \n",
      " |      Pick single key:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |      \n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |      \n",
      " |      Pick list of keys:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Any\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |      \n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str,\n",
      " |                  json=as_json,\n",
      " |                  bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |      \n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
      " |      \n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |      \n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |      \n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind asynchronous lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Asynchronously called before the runnable starts running.\n",
      " |      on_end: Asynchronously called after the runnable finishes running.\n",
      " |      on_error: Asynchronously called if the runnable throws an error.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          import time\n",
      " |      \n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |      \n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637053+00:00\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637150+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638305+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638383+00:00\n",
      " |          Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n",
      " |          Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n",
      " |          Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:35.640534+00:00\n",
      " |          Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:37.640574+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:37.640654+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:39.641751+00:00\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Iterator\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |      \n",
      " |      \n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |      \n",
      " |      \n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |      \n",
      " |      \n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |                  )\n",
      " |              print(''.join(runnable.stream({}))) #foo bar\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |      \n",
      " |          import time\n",
      " |      \n",
      " |          def test_runnable(time_to_sleep : int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |      \n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |      \n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |      \n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          count = 0\n",
      " |      \n",
      " |      \n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                   pass\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |      \n",
      " |          assert (count == 2)\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  name = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n"
     ]
    }
   ],
   "source": [
    "help(llm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:55:55.201777Z",
     "start_time": "2024-06-28T02:55:55.169342Z"
    }
   },
   "id": "78d985880d357163",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Config',\n 'InputType',\n 'OutputType',\n '__abstractmethods__',\n '__annotations__',\n '__call__',\n '__class__',\n '__class_getitem__',\n '__class_vars__',\n '__config__',\n '__custom_root_type__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__exclude_fields__',\n '__fields__',\n '__fields_set__',\n '__format__',\n '__ge__',\n '__get_validators__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__include_fields__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__json_encoder__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__or__',\n '__orig_bases__',\n '__parameters__',\n '__post_root_validators__',\n '__pre_root_validators__',\n '__pretty__',\n '__private_attributes__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__repr_args__',\n '__repr_name__',\n '__repr_str__',\n '__rich_repr__',\n '__ror__',\n '__schema_cache__',\n '__setattr__',\n '__setstate__',\n '__signature__',\n '__sizeof__',\n '__slots__',\n '__str__',\n '__subclasshook__',\n '__try_update_forward_refs__',\n '__validators__',\n '__weakref__',\n '_abatch_with_config',\n '_abc_impl',\n '_acall_with_config',\n '_agenerate',\n '_agenerate_helper',\n '_all_required_field_names',\n '_astream',\n '_atransform_stream_with_config',\n '_batch_with_config',\n '_calculate_keys',\n '_call_async',\n '_call_with_config',\n '_chunk_to_generation',\n '_convert_input',\n '_copy_and_set_values',\n '_decompose_class',\n '_default_params',\n '_enforce_dict_if_root',\n '_generate',\n '_generate_helper',\n '_generation_from_qwen_resp',\n '_get_run_ids_list',\n '_get_value',\n '_identifying_params',\n '_init_private_attributes',\n '_invocation_params',\n '_is_protocol',\n '_iter',\n '_llm_type',\n '_stream',\n '_transform_stream_with_config',\n 'abatch',\n 'abatch_as_completed',\n 'agenerate',\n 'agenerate_prompt',\n 'ainvoke',\n 'apredict',\n 'apredict_messages',\n 'assign',\n 'astream',\n 'astream_events',\n 'astream_log',\n 'atransform',\n 'batch',\n 'batch_as_completed',\n 'bind',\n 'cache',\n 'callback_manager',\n 'callbacks',\n 'client',\n 'config_schema',\n 'config_specs',\n 'configurable_alternatives',\n 'configurable_fields',\n 'construct',\n 'copy',\n 'custom_get_token_ids',\n 'dashscope_api_key',\n 'dict',\n 'from_orm',\n 'generate',\n 'generate_prompt',\n 'get_graph',\n 'get_input_schema',\n 'get_lc_namespace',\n 'get_name',\n 'get_num_tokens',\n 'get_num_tokens_from_messages',\n 'get_output_schema',\n 'get_prompts',\n 'get_token_ids',\n 'input_schema',\n 'invoke',\n 'is_lc_serializable',\n 'json',\n 'lc_attributes',\n 'lc_id',\n 'lc_secrets',\n 'map',\n 'max_retries',\n 'metadata',\n 'model_kwargs',\n 'model_name',\n 'name',\n 'output_schema',\n 'parse_file',\n 'parse_obj',\n 'parse_raw',\n 'pick',\n 'pipe',\n 'predict',\n 'predict_messages',\n 'raise_deprecation',\n 'save',\n 'schema',\n 'schema_json',\n 'set_verbose',\n 'stream',\n 'streaming',\n 'tags',\n 'to_json',\n 'to_json_not_implemented',\n 'top_p',\n 'transform',\n 'update_forward_refs',\n 'validate',\n 'validate_environment',\n 'verbose',\n 'with_alisteners',\n 'with_config',\n 'with_fallbacks',\n 'with_listeners',\n 'with_retry',\n 'with_structured_output',\n 'with_types']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(llm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:56:04.075778Z",
     "start_time": "2024-06-28T02:56:04.064305Z"
    }
   },
   "id": "a9f42e3d44d44051",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_di = Tongyi(temperature=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:57:44.143407Z",
     "start_time": "2024-06-28T02:57:44.138103Z"
    }
   },
   "id": "6e280d871f9122a3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "s = \"请帮我写一副关于龙年的对联\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:58:19.584103Z",
     "start_time": "2024-06-28T02:58:19.579993Z"
    }
   },
   "id": "60f0a629ef82238f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上联：龙舞长空，瑞气盈门福满堂\n",
      "下联：年华似锦，盛世如歌春意浓\n",
      "横批：龙年大吉，喜结四海祥云"
     ]
    },
    {
     "data": {
      "text/plain": "'上联：龙舞长空，瑞气盈门福满堂\\n下联：年华似锦，盛世如歌春意浓\\n横批：龙年大吉，喜结四海祥云'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:58:36.421126Z",
     "start_time": "2024-06-28T02:58:33.070143Z"
    }
   },
   "id": "c3e248f2e95e51ab",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'上联：龙舞长空，瑞气盈门喜庆春\\n下联：年华似锦，福星高照乐融融\\n横批：龙年大吉，盛世繁华'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_di.invoke(s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T02:59:00.489989Z",
     "start_time": "2024-06-28T02:58:56.871579Z"
    }
   },
   "id": "4a7ce50e81351b6b",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_ga = Tongyi(temperature=0.9)\n",
    "res = llm_ga.generate([\"写一首唐诗绝句\",\"写一首宋词《贺新郎》\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:02:30.447769Z",
     "start_time": "2024-06-28T03:02:16.692461Z"
    }
   },
   "id": "2303852b2fca28f1",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春风吹柳绿如丝，桃花绽放映朝辉。\n",
      "江水潺潺流不息，渔舟唱晚入画帷。\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'finish_reason': 'stop',\n 'request_id': '85fd13ca-2936-92aa-b578-317b75cdf056',\n 'token_usage': {'input_tokens': 14, 'output_tokens': 29, 'total_tokens': 43}}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(res.generations[0][0].text)\n",
    "res.generations[0][0].generation_info"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:05:16.519054Z",
     "start_time": "2024-06-28T03:05:16.511883Z"
    }
   },
   "id": "5b6f59afb6ad0aeb",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《贺新郎·赋壮志豪情》\n",
      "\n",
      "举头望明月，豪情盈胸臆。万里江山，英雄何处觅知音，剑光如龙舞。醉里挑灯看剑，梦回吹角连营，壮志凌云深。 \n",
      "\n",
      "烽火戏诸侯，世事如棋局。谁人挥毫，书尽人间疾苦，笔下生风雷。铁马冰河入梦来，金戈铁马，气吞万里如虎，誓将功名铸。\n",
      "\n",
      "倚天照海花无数，世间英豪争锋处。笑谈间，樯橹灰飞烟灭，谈笑风生无惧。待到功成名就时，回首往事，一杯浊酒祭英魂，壮志长存，不负韶华年少，贺此生，新郎英勇，词韵犹在耳。\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'finish_reason': 'stop',\n 'request_id': 'bd16be83-9b58-9a35-a083-5c7341bbdd62',\n 'token_usage': {'input_tokens': 17,\n  'output_tokens': 187,\n  'total_tokens': 204}}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(res.generations[1][0].text)\n",
    "res.generations[1][0].generation_info"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:05:32.270101Z",
     "start_time": "2024-06-28T03:05:32.262947Z"
    }
   },
   "id": "dfd07f51c8474166",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'很抱歉，由于您没有提供具体的贺新郎词，我无法直接判断是否符合词牌格律。请您提供完整的词，我会帮您分析是否符合贺新郎的词牌格律。贺新郎，又名《金缕曲》《贺新凉》，是宋代刘克庄创制的词牌，双调，一百十六字，上下片各十句六仄韵。'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_ga.invoke(\"这首贺新郎是否符合词牌格律？\") # 显然模型调用没法识别历史提问上下文"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:08:21.019258Z",
     "start_time": "2024-06-28T03:08:15.379850Z"
    }
   },
   "id": "1fa809c58fb23790",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "我希望你能充当新公司的命名顾问。\n",
    "一个生产{product}的公司的好名字是什么？\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt = prompt_template.format(product=\"彩色袜子\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:13:20.242371Z",
     "start_time": "2024-06-28T03:13:20.137140Z"
    }
   },
   "id": "90249a86c4a086b8",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为你的彩色袜子公司取名是一项有趣的任务。以下是一些创意的建议：\n",
      "\n",
      "1. \"RainbowSteps\"\n",
      "2. \"SoleSplash\"\n",
      "3. \"VibrantToes\"\n",
      "4. \"ColorCraze Socks\"\n",
      "5. \"PiedPiper Socks\"\n",
      "6. \"FiestaFeet\"\n",
      "7. \"SpectrumSocks\"\n",
      "8. \"ChromaticSoles\"\n",
      "9. \"ZestyThreads\"\n",
      "10. \"JazzyJamboree\"\n",
      "\n",
      "在选择名字时，考虑公司的品牌定位、目标市场和你希望传达的情感。确保名字易于记忆，且在商标搜索中是独特的。祝你的新公司取得成功！\n"
     ]
    }
   ],
   "source": [
    "print(llm_ga.invoke(prompt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:13:31.140130Z",
     "start_time": "2024-06-28T03:13:21.424569Z"
    }
   },
   "id": "226ba386ed6086f3",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"], \n",
    "    template=\"给我讲一个{adjective}的关于{content}的笑话。\"\n",
    ")\n",
    "prompt = multiple_input_prompt.format(adjective=\"有趣的\", content=\"小鸡\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:16:34.981904Z",
     "start_time": "2024-06-28T03:16:34.977536Z"
    }
   },
   "id": "6b843becc427c1c2",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，这里有一个关于小鸡的笑话：\n",
      "\n",
      "为什么小鸡总是这么乐观？\n",
      "\n",
      "因为它们每天都能看到新的日出（蛋）！\n"
     ]
    }
   ],
   "source": [
    "print(llm_ga.invoke(prompt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T03:16:44.750025Z",
     "start_time": "2024-06-28T03:16:43.112393Z"
    }
   },
   "id": "e32e3e4d1e9aa24b",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "Word: big\n",
      "Antonym: \n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# 首先，创建Few Shot示例列表\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "# 接下来，我们指定用于格式化示例的模板。\n",
    "# 我们使用`PromptTemplate`类来实现这个目的。\n",
    "example_formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "# 最后，创建`FewShotPromptTemplate`对象。\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # 这些是我们要插入到提示中的示例。\n",
    "    examples=examples,\n",
    "    # 这是我们在将示例插入到提示中时要使用的格式。\n",
    "    example_prompt=example_prompt_template,\n",
    "    # 前缀是出现在提示中示例之前的一些文本。\n",
    "    # 通常，这包括一些说明。\n",
    "    prefix=\"Give the antonym of every input\\n\",\n",
    "    # 后缀是出现在提示中示例之后的一些文本。\n",
    "    # 通常，这是用户输入的地方。\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    # 输入变量是整个提示期望的变量。\n",
    "    input_variables=[\"input\"],\n",
    "    # 示例分隔符是我们将前缀、示例和后缀连接在一起的字符串。\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "# 现在，我们可以使用`format`方法生成一个提示。\n",
    "prompt = few_shot_prompt.format(input=\"big\")\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:11:28.551017Z",
     "start_time": "2024-07-01T02:11:28.440822Z"
    }
   },
   "id": "9754cf082b1f449e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "死气 (sǐ qì) - literally means \"dead air\" or \"lack of vitality\" in Chinese"
     ]
    },
    {
     "data": {
      "text/plain": "'死气 (sǐ qì) - literally means \"dead air\" or \"lack of vitality\" in Chinese'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(few_shot_prompt.format(input=\"生机\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:15:10.683681Z",
     "start_time": "2024-07-01T02:15:09.006944Z"
    }
   },
   "id": "577b6404ecef6bad",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello' id='run-9ea803e5-136c-40ab-b9cd-d3d557d85898'\n",
      "content='!' id='run-9ea803e5-136c-40ab-b9cd-d3d557d85898'\n",
      "content=' How' id='run-9ea803e5-136c-40ab-b9cd-d3d557d85898'\n",
      "content=' can I assist you today' id='run-9ea803e5-136c-40ab-b9cd-d3d557d85898'\n",
      "content='?' response_metadata={'finish_reason': 'stop', 'request_id': 'd3ccaf75-7658-9e0a-b981-c343e0314321', 'token_usage': {'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18}} id='run-9ea803e5-136c-40ab-b9cd-d3d557d85898'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_ty = ChatTongyi(\n",
    "    streaming=True,\n",
    ")\n",
    "res = chat_ty.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "try:\n",
    "    for chunk in res:\n",
    "        print(chunk)\n",
    "except TypeError as e:\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:21:59.781255Z",
     "start_time": "2024-07-01T02:21:58.826705Z"
    }
   },
   "id": "b1b1ce221c163fe4",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res = chat_ty.stream([HumanMessage(content=\"帮我写副对联\")], streaming=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:23:04.729821Z",
     "start_time": "2024-07-01T02:23:04.724625Z"
    }
   },
   "id": "f51809792fad0f2f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然\n",
      "可以\n",
      "，\n",
      "这里有一副适合春节\n",
      "的对联供您参考：\n",
      "\n",
      "上\n",
      "联：红梅傲雪春来到\n",
      "，富贵吉祥喜盈门。\n",
      "下\n",
      "联：绿竹报岁福无\n",
      "边，平安如意乐满堂。\n",
      "\n",
      "横批：新春纳福\n",
      "\n",
      "如果您\n",
      "有其他主题或场合的需求，也\n",
      "请告诉我，我会尽力为您定制。\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for chunk in res:\n",
    "        print(chunk.content)\n",
    "except TypeError as e:\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:23:08.430648Z",
     "start_time": "2024-07-01T02:23:05.843217Z"
    }
   },
   "id": "2946f5efa3c7a734",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"我希望你能充当新公司的命名顾问。一个生产{product}的公司好的中文名字是什么,请给出五个选项？\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:28:25.739814Z",
     "start_time": "2024-07-01T02:28:25.735529Z"
    }
   },
   "id": "747ad659792b5b23",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:34:36.742137Z",
     "start_time": "2024-07-01T02:34:36.738055Z"
    }
   },
   "id": "99ca3c8cecede04b",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 彩韵袜业：这个名字融合了\"彩\"（彩色）和\"韵\"（韵律，寓意袜子的多样性和时尚感），体现出产品的特色。\n",
      "\n",
      "2. 明彩足下：明彩代表明亮的色彩，足下则直接指代袜子，寓意穿上公司的彩色袜子能让人的步伐更加生动。\n",
      "\n",
      "3. 色舞袜坊：色舞寓意色彩的灵动与活力，坊则给人一种亲切和手工艺的感觉，适合小型或有独特工艺的袜子公司。\n",
      "\n",
      "4. 七彩虹袜：七彩虹代表了丰富的颜色，直接关联到公司的主要产品，同时也传达出快乐、积极的品牌形象。\n",
      "\n",
      "5. 魅彩袜语：魅彩表示袜子的吸引力和色彩魅力，\"袜语\"则暗示了袜子可以作为表达个性的一种方式，富有创意。"
     ]
    },
    {
     "data": {
      "text/plain": "'1. 彩韵袜业：这个名字融合了\"彩\"（彩色）和\"韵\"（韵律，寓意袜子的多样性和时尚感），体现出产品的特色。\\n\\n2. 明彩足下：明彩代表明亮的色彩，足下则直接指代袜子，寓意穿上公司的彩色袜子能让人的步伐更加生动。\\n\\n3. 色舞袜坊：色舞寓意色彩的灵动与活力，坊则给人一种亲切和手工艺的感觉，适合小型或有独特工艺的袜子公司。\\n\\n4. 七彩虹袜：七彩虹代表了丰富的颜色，直接关联到公司的主要产品，同时也传达出快乐、积极的品牌形象。\\n\\n5. 魅彩袜语：魅彩表示袜子的吸引力和色彩魅力，\"袜语\"则暗示了袜子可以作为表达个性的一种方式，富有创意。'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"彩色袜子\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:28:56.754979Z",
     "start_time": "2024-07-01T02:28:47.725839Z"
    }
   },
   "id": "20b119ae34cad2c9",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以，以下是我为这个多元产品公司提出的五个中文名称建议：\n",
      "1. 跨悦科技（跨越生活，悦享品质，同时“跨悦”也能暗示产品的多样性）\n",
      "2. 着装未来（\"着装\"涵盖了衣物类，\"未来\"则与电脑的高科技感相呼应）\n",
      "3. 通途舒步（\"通途\"象征电脑带来的信息通路，\"舒步\"则关乎鞋类产品，整体寓意舒适前行）\n",
      "4. 多元智足（\"多元\"代表产品种类丰富，\"智\"对应电脑，\"足\"对应鞋子，也寓意智慧生活）\n",
      "5. 缘步云袜（\"缘步\"有随缘而行之意，\"云\"则可关联到电脑的云计算，\"袜\"直接指代该产品线）\n",
      "\n",
      "请根据公司的品牌定位、目标市场和喜好来选择最适合的名字。"
     ]
    },
    {
     "data": {
      "text/plain": "'当然可以，以下是我为这个多元产品公司提出的五个中文名称建议：\\n\\n1. 跨悦科技（跨越生活，悦享品质，同时“跨悦”也能暗示产品的多样性）\\n2. 着装未来（\"着装\"涵盖了衣物类，\"未来\"则与电脑的高科技感相呼应）\\n3. 通途舒步（\"通途\"象征电脑带来的信息通路，\"舒步\"则关乎鞋类产品，整体寓意舒适前行）\\n4. 多元智足（\"多元\"代表产品种类丰富，\"智\"对应电脑，\"足\"对应鞋子，也寓意智慧生活）\\n5. 缘步云袜（\"缘步\"有随缘而行之意，\"云\"则可关联到电脑的云计算，\"袜\"直接指代该产品线）\\n\\n请根据公司的品牌定位、目标市场和喜好来选择最适合的名字。'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"product\": \"袜子\"},\n",
    "    {\"product\": \"电脑\"},\n",
    "    {\"product\": \"男鞋\"}\n",
    "]\n",
    "chain.invoke(input_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:35:10.825476Z",
     "start_time": "2024-07-01T02:35:00.568883Z"
    }
   },
   "id": "e506807b2bfd4268",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以，以下是我为您的袜子公司提出的五个中文名字建议：\n",
      "1. \"舒步袜业\"：这个名字强调了袜子的舒适性和适合行走的特点，同时也表达了轻松愉快的步履之意。\n",
      "2. \"纤丝韵\"：这个名字突出了袜子材质的细腻和优雅，韵字则带出了一种时尚和节奏感。\n",
      "3. \"足下温馨\"：简洁易记，寓意袜子带来的温暖和关怀，让人感到亲切。\n",
      "4. \"行云袜\"：借用“行云流水”的成语，寓意穿着者的步伐轻盈自在，同时展现了袜子的高品质。\n",
      "5. \"彩织生活\"：强调袜子的多彩选择和它为日常生活增添的色彩，也暗示了公司产品的丰富多样。\n",
      "\n",
      "希望这些建议能对您有所帮助！当然可以，以下是我为您的电脑生产公司提出的五个可能的中文名字建议：\n",
      "\n",
      "1. \"智启科技\"：这个名字强调了智慧和创新的启动力量，代表电脑技术的智能开启未来。\n",
      "2. \"灵翼计算机\"：\"灵翼\"寓意灵活、快速，象征电脑的高效性能和无限可能。\n",
      "3. \"迅捷芯动\"：强调电脑的运行速度和处理器的强劲动力，同时也表达了动态和活力。\n",
      "4. \"明略电子\"：\"明略\"意味着明智的策略和清晰的视野，暗示公司的前瞻性和产品的高质量。\n",
      "5. \"睿智未来\"：突出了电脑作为智慧工具的角色，以及对未来智能生活的引领。\n",
      "\n",
      "这些名字都包含了积极的寓意，希望能给您一些灵感。在选择时，还可以考虑与公司的品牌理念、目标市场和产品特性相匹配的因素。当然可以，以下是我为您的男鞋公司提出的五个可能的中文名字建议：\n",
      "\n",
      "1. 雄步天下：这个名字寓含了男性稳健、自信的步伐，同时也暗示了公司的产品能够帮助消费者走遍天下。\n",
      "2. 驭风绅士：驭风象征速度和力量，绅士则代表了品质和优雅，适合定位中高端市场的男鞋品牌。\n",
      "3. 革履先生：以“革”突出鞋子的主要材质，而“先生”则给人一种稳重、有品位的感觉。\n",
      "4. 峻履前行：峻字寓意坚韧不拔，前行则象征着前进的动力，适合强调产品质量和进取精神的品牌。\n",
      "5. 稳行者：稳字体现了鞋子的舒适性和稳定性，行者则代表了穿着者的旅行和生活态度。\n",
      "\n",
      "每个名字都有其独特的含义，希望能帮到您。在选择时，还需要考虑品牌的定位、目标市场以及想要传达的品牌形象。"
     ]
    }
   ],
   "source": [
    "for input in input_list:\n",
    "    chain.invoke(input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T02:39:17.542504Z",
     "start_time": "2024-07-01T02:38:51.136775Z"
    }
   },
   "id": "7dcb063fe5bc4eb2",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "PromptTemplate(input_variables=['subject'], partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'}, template='请列出五个 {subject}.\\n{format_instructions}')"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"请列出五个 {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T03:01:40.163348Z",
     "start_time": "2024-07-01T03:01:40.155219Z"
    }
   },
   "id": "4b350ad3994019d9",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "泰山, 黄山, 衡山, 华山, 嵩山"
     ]
    },
    {
     "data": {
      "text/plain": "['泰山', '黄山', '衡山', '华山', '嵩山']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input0 = prompt.format(subject=\"中国的名山\")\n",
    "output = llm.invoke(input0)\n",
    "res0=output_parser.parse(output)\n",
    "res0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T03:03:16.068265Z",
     "start_time": "2024-07-01T03:03:14.624644Z"
    }
   },
   "id": "674632333742348e",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 其他解析器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15d73bb712ab4e34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser,StructuredOutputParser\n",
    "# 日期时间解析器:DatetimeOutputParser\n",
    "# 结构化输出解析器:StructuredOutputParser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ccedd683be54725"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 历史消息"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29f0fae6eab2ec37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory\n",
    "\n",
    "大多数的 LLM 应用程序都会有一个会话接口，允许我们和 LLM 进行多轮的对话，并有一定的上下文记忆能力。但实际上，模型本身是不会记忆任何上下文的，只能依靠用户本身的输入去产生输出。而实现这个记忆功能，就需要额外的模块去保存我们和模型对话的上下文信息，然后在下一次请求时，把所有的历史信息都输入给模型，让模型输出最终结果。\n",
    "\n",
    "而在 LangChain 中，提供这个功能的模块就称为 Memory，用于存储用户和模型交互的历史信息。在 LangChain 中根据功能和返回值的不同，会有多种不同的 Memory 类型，主要可以分为以下几个类别：\n",
    "\n",
    "* 对话缓冲区内存（ConversationBufferMemory），最基础的内存模块，用于存储历史的信息\n",
    "* 对话缓冲器窗口内存（ConversationBufferWindowMemory），只保存最后的 K 轮对话的信息，因此这种内存空间使用会相对较少\n",
    "* 对话摘要内存（ConversationSummaryMemory），这种模式会对历史的所有信息进行抽取，生成摘要信息，然后将摘要信息作为历史信息进行保存。\n",
    "* 对话摘要缓存内存（ConversationSummaryBufferMemory），这个和上面的作用基本一致，但是有最大 token 数的限制，达到这个最大 token 数的时候就会进行合并历史信息生成摘要\n",
    "\n",
    "值得注意的是，对话摘要内存的设计出发点就是语言模型能支持的上下文长度是有限的（一般是 2048），超过了这个长度的数据天然的就被截断了。这个类会根据对话的轮次进行合并，默认值是 2，也就是每 2 轮就开启一次调用 LLM 去合并历史信息。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c0c80c07043dc2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 链外管理内存\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T05:41:15.303877Z",
     "start_time": "2024-07-01T05:41:15.280294Z"
    }
   },
   "id": "3061ed7689a2b744",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001B[0m\n",
      "Hello! How can I assist you today? I'm here to provide information, answer questions, or engage in some interesting chatter. Feel free to ask me anything!\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Hello! How can I assist you today? I'm here to provide information, answer questions, or engage in some interesting chatter. Feel free to ask me anything!\""
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对话缓冲区内存（ConversationBufferMemory）\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hi there!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T03:10:39.023077Z",
     "start_time": "2024-07-01T03:10:36.598619Z"
    }
   },
   "id": "9ef8f8cd5b216e8a",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How can I assist you today? I'm here to provide information, answer questions, or engage in some interesting chatter. Feel free to ask me anything!\n",
      "Human: 你是谁\n",
      "AI:\u001B[0m\n",
      "我是一个人工智能助手，被设计来帮助用户解答问题、提供信息或进行友好的对话。你可以把我想象成一个知识丰富的聊天机器人，随时准备与你交流。\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'我是一个人工智能助手，被设计来帮助用户解答问题、提供信息或进行友好的对话。你可以把我想象成一个知识丰富的聊天机器人，随时准备与你交流。'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"你是谁\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T03:13:44.682274Z",
     "start_time": "2024-07-01T03:13:41.588114Z"
    }
   },
   "id": "21e77f0e9ba6535c",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='Hi there!'),\n AIMessage(content=\"Hello! How can I assist you today? I'm here to provide information, answer questions, or engage in some interesting chatter. Feel free to ask me anything!\"),\n HumanMessage(content='你是谁'),\n AIMessage(content='我是一个人工智能助手，被设计来帮助用户解答问题、提供信息或进行友好的对话。你可以把我想象成一个知识丰富的聊天机器人，随时准备与你交流。')]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.chat_memory.messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T03:14:23.488477Z",
     "start_time": "2024-07-01T03:14:23.482041Z"
    }
   },
   "id": "ea6fdf4f485e68f9",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'type': 'human',\n  'data': {'content': 'Hi there!',\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'human',\n   'name': None,\n   'id': None,\n   'example': False}},\n {'type': 'ai',\n  'data': {'content': \"Hello! How can I assist you today? I'm here to provide information, answer questions, or engage in some interesting chatter. Feel free to ask me anything!\",\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'ai',\n   'name': None,\n   'id': None,\n   'example': False,\n   'tool_calls': [],\n   'invalid_tool_calls': [],\n   'usage_metadata': None}},\n {'type': 'human',\n  'data': {'content': '你是谁',\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'human',\n   'name': None,\n   'id': None,\n   'example': False}},\n {'type': 'ai',\n  'data': {'content': '我是一个人工智能助手，被设计来帮助用户解答问题、提供信息或进行友好的对话。你可以把我想象成一个知识丰富的聊天机器人，随时准备与你交流。',\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'ai',\n   'name': None,\n   'id': None,\n   'example': False,\n   'tool_calls': [],\n   'invalid_tool_calls': [],\n   'usage_metadata': None}}]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "import json\n",
    "dicts = messages_to_dict(conversation.memory.chat_memory.messages)\n",
    "dicts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T05:43:29.940634Z",
     "start_time": "2024-07-01T05:43:29.934432Z"
    }
   },
   "id": "42f5b2a774eee838",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"./memory\", 'wb')\n",
    "pickle.dump(dicts,f)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T05:44:01.534104Z",
     "start_time": "2024-07-01T05:44:01.527134Z"
    }
   },
   "id": "3aa74b30865720cd",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'type': 'human',\n  'data': {'content': 'Hi there!',\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'human',\n   'name': None,\n   'id': None,\n   'example': False}},\n {'type': 'ai',\n  'data': {'content': \"Hello! How can I assist you today? I'm here to provide information, answer questions, or engage in some interesting chatter. Feel free to ask me anything!\",\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'ai',\n   'name': None,\n   'id': None,\n   'example': False,\n   'tool_calls': [],\n   'invalid_tool_calls': [],\n   'usage_metadata': None}},\n {'type': 'human',\n  'data': {'content': '你是谁',\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'human',\n   'name': None,\n   'id': None,\n   'example': False}},\n {'type': 'ai',\n  'data': {'content': '我是一个人工智能助手，被设计来帮助用户解答问题、提供信息或进行友好的对话。你可以把我想象成一个知识丰富的聊天机器人，随时准备与你交流。',\n   'additional_kwargs': {},\n   'response_metadata': {},\n   'type': 'ai',\n   'name': None,\n   'id': None,\n   'example': False,\n   'tool_calls': [],\n   'invalid_tool_calls': [],\n   'usage_metadata': None}}]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts_load = pickle.load(open(\"./memory\", \"rb\"))\n",
    "dicts_load"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T05:44:50.257650Z",
     "start_time": "2024-07-01T05:44:50.250103Z"
    }
   },
   "id": "d9f7ae23b92ba10b",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How can I assist you today? I'm here to provide information, answer questions, or engage in some interesting chatter. Feel free to ask me anything!\n",
      "Human: 你是谁\n",
      "AI: 我是一个人工智能助手，被设计来帮助用户解答问题、提供信息或进行友好的对话。你可以把我想象成一个知识丰富的聊天机器人，随时准备与你交流。\n",
      "Human: 我回来了\n",
      "AI:\u001B[0m\n",
      "欢迎回来！有什么新话题或问题想要讨论吗？或者你需要我帮助解答上次我们谈话时留下的任何疑问吗？\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'欢迎回来！有什么新话题或问题想要讨论吗？或者你需要我帮助解答上次我们谈话时留下的任何疑问吗？'"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向对话添加记忆\n",
    "new_messages = messages_from_dict(dicts_load)\n",
    "retrieved_chat_history = ChatMessageHistory(messages=new_messages)\n",
    "retrieved_memory = ConversationBufferMemory(chat_memory=retrieved_chat_history)\n",
    "\n",
    "conversation_reload = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=retrieved_memory\n",
    ")\n",
    "conversation_reload.predict(input=\"我回来了\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T05:45:45.496598Z",
     "start_time": "2024-07-01T05:45:43.475649Z"
    }
   },
   "id": "5b976c1c1823dae6",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2c793c5c5d00e80d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 检索增强 与 文本向量器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38165cbdc18f6830"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 文档导入"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53a9c20e5c2a7a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "model_ty = Tongyi(temperature=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:02:35.390527Z",
     "start_time": "2024-07-01T06:02:35.386324Z"
    }
   },
   "id": "dd945edcf2e15761",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 不用print函数，直接用变量名称打印输出\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T06:55:31.564333Z",
     "start_time": "2024-07-03T06:55:31.554258Z"
    }
   },
   "id": "f6d20d7d0600847e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "list"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "129"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "langchain_core.documents.base.Document"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader_pdf = PyPDFLoader(r\"D:\\workspace\\收发文件\\新型冠状病毒肺炎防控方案（第八版）.pdf\")\n",
    "docs_pdf = loader_pdf.load()\n",
    "type(docs_pdf)\n",
    "len(docs_pdf)\n",
    "type(docs_pdf[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:06:17.566278Z",
     "start_time": "2024-07-01T06:06:13.524214Z"
    }
   },
   "id": "c5c5b1337e26157b",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'source': 'D:\\\\workspace\\\\收发文件\\\\新型冠状病毒肺炎防控方案（第八版）.pdf', 'page': 0}"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "'—2—新型冠状病毒肺炎防控方案\\n（第八版）\\n为指导各地做好新型冠状病毒肺炎（以下简称新冠肺炎，\\nCOVID-19）疫情的常态化防控工作，全面落实“外防输入、内\\n防反弹”的防控策略，根据新冠肺炎乙类传染'"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_pdf[0].metadata\n",
    "docs_pdf[0].page_content[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:07:06.993239Z",
     "start_time": "2024-07-01T06:07:06.985218Z"
    }
   },
   "id": "76e58d43ead0b0ba",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "{'source': 'D:\\\\workspace\\\\收发文件\\\\李卅移交资料.txt'}"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "'李卅移交资料：在itdevc07-EKB开发机上（10.66.58.121，用户名：itdevc07\\\\lcadmin，密码：2020@ekb），\\n目录地址：D:\\\\DE-IoT 柴油机物联网.7z.00'"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader_txt = TextLoader(r'D:\\workspace\\收发文件\\李卅移交资料.txt', encoding='utf8')\n",
    "docs_txt = loader_txt.load()\n",
    "len(docs_txt)\n",
    "\n",
    "docs_txt[0].metadata\n",
    "docs_txt[0].page_content[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:10:23.248688Z",
     "start_time": "2024-07-01T06:10:23.236668Z"
    }
   },
   "id": "5a3dfc7eca2f5aae",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "{'source': 'https://news.ifeng.com/c/8Y3TlIcTsj0',\n 'title': '足协原副主席王登峰被判17年_凤凰网',\n 'description': '足协原副主席王登峰被判17年',\n 'language': 'zh'}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "'足协原副主席王登峰被判17年_凤凰网\\n\\n\\n\\n首页资讯视频直播凤凰卫视财经娱乐体育时尚汽车房产科技读书文化历史军事旅游佛教更多国学数码健康家居公益教育酒业美食资讯 > 大陆 > 正文足协原副主席王登峰被'"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "WEB_URL = \"https://news.ifeng.com/c/8Y3TlIcTsj0\"\n",
    "loader_html = WebBaseLoader(WEB_URL)\n",
    "docs_html = loader_html.load()\n",
    "\n",
    "len(docs_html)\n",
    "docs_html[0].metadata\n",
    "docs_html[0].page_content[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:11:24.021527Z",
     "start_time": "2024-07-01T06:11:22.627594Z"
    }
   },
   "id": "7a9903317162d847",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "' \\n* RecursiveCharacterTextSplitter 递归字符文本分割\\nRecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\\n    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\\nRecursiveCharacterTextSplitter需要关注的是4个参数：\\n\\n* separators - 分隔符字符串数组\\n* chunk_size - 每个文档的字符数量限制\\n* chunk_overlap - 两份文档重叠区域的长度\\n* length_function - 长度计算函数\\n'"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 导入递归字符文本分割器\n",
    "text_splitter_txt = RecursiveCharacterTextSplitter(chunk_size = 384, chunk_overlap = 0, separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"。\", \"，\"])\n",
    "# 导入文本\n",
    "documents_txt = text_splitter_txt.split_documents(docs_txt)\n",
    "len(documents_txt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:12:21.582057Z",
     "start_time": "2024-07-01T06:12:21.544949Z"
    }
   },
   "id": "5f0e69946c83af57",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'李卅移交资料：在itdevc07-EKB开发机上（10.66.58.121，用户名：itdevc07\\\\lcadmin，密码：2020@ekb），\\n目录地址：D:\\\\DE-IoT 柴油机物联网.7z.001~008。8个压缩文件用7-zip解压软件任意解压一个即可。\\n主要资料位于解压后的 ./clickhouse 和./DMP开发 两个目录中。'"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_txt[0].page_content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:12:50.394119Z",
     "start_time": "2024-07-01T06:12:50.388610Z"
    }
   },
   "id": "dc22cbea0d584dd3",
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "___"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "853ba8e89f3bca7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_transformer  = SentenceTransformer('D:\\workspace\\project\\LLM\\models\\m3e-base')\n",
    "sentences = [\n",
    "    '* Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem',\n",
    "    '* Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练',\n",
    "    '* Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one'\n",
    "]\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb = sentence_transformer.encode(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T07:05:40.006327Z",
     "start_time": "2024-07-01T07:05:38.506585Z"
    }
   },
   "id": "961c7c6f8c4f0530",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query1 = \"狗\"\n",
    "query2 = \"猫\"\n",
    "query3 = \"雨\"\n",
    "# 通过对应的 embedding 类生成 query 的 embedding。\n",
    "emb1 = sentence_transformer.encode(query1)\n",
    "emb2 = sentence_transformer.encode(query2)\n",
    "emb3 = sentence_transformer.encode(query3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:24:38.439169Z",
     "start_time": "2024-07-01T06:24:38.322068Z"
    }
   },
   "id": "e95fca3f90f541e1",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "320.04456"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "302.9192"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "300.03458"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 将返回结果转成 numpy 的格式，便于后续计算\n",
    "emb1 = np.array(emb1)\n",
    "emb2 = np.array(emb2)\n",
    "emb3 = np.array(emb3)\n",
    "\n",
    "np.dot(emb1, emb2)\n",
    "np.dot(emb3, emb2)\n",
    "np.dot(emb1, emb3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T06:24:51.808658Z",
     "start_time": "2024-07-01T06:24:51.798245Z"
    }
   },
   "id": "f8354e57d0a3fc5a",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "D:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# sentenceTransformer = SentenceTransformer('D:\\workspace\\project\\LLM\\models\\m3e-base')\n",
    "embedding = SentenceTransformerEmbeddings(model_name='D:\\workspace\\project\\LLM\\models\\m3e-base')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T05:36:53.455502Z",
     "start_time": "2024-07-03T05:36:44.303231Z"
    }
   },
   "id": "bc84dfacbe2776f8",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "WEB_URL = \"https://news.ifeng.com/c/8Y3TlIcTsj0\"\n",
    "loader_html = WebBaseLoader(WEB_URL)\n",
    "docs_html = loader_html.load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T05:36:22.569898Z",
     "start_time": "2024-07-03T05:36:20.790865Z"
    }
   },
   "id": "5ea522758f17b518",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='足协原副主席王登峰被判17年_凤凰网\\n\\n\\n\\n首页资讯视频直播凤凰卫视财经娱乐体育时尚汽车房产科技读书文化历史军事旅游佛教更多国学数码健康家居公益教育酒业美食资讯 > 大陆 > 正文足协原副主席王登峰被判17年山东高法2024年03月18日 16:55:00 2024年3月18日上午，山东省德州市中级人民法院一审公开宣判教育部体育卫生与艺术教育司原司长王登峰贪污、受贿案，以贪污罪判处其有期徒刑十四年，并处罚金人民币四百万元；以受贿罪判处其有期徒刑十一年，并处罚金人民币一百万元，决定执行有期徒刑十七年，并处罚金人民币五百万元。查扣在案的贪污赃款依法返还被害单位，受贿犯罪违法所得及孳息依法没收，上缴国库。经审理查明：一、贪污罪。2016年6月至2022年8月，被告人王登峰利用担任教育部体育卫生与艺术教育司司长等的职务便利，单独或伙同他人，采取侵吞、骗取等手段，非法占有公共财物共计4670.13万余元，其中180.49万余元未遂。二、受贿罪。2006年1月至2022年春节前，被告人王登峰利用担任教育部体育卫生与艺术教育司司长等职务便利，为他人在项目承揽、赛事承办等方面谋取利益，单独或伙同他人收受相关单位或个人给予的财物，共计折合人民币966.469万元。德州市中级人民法院认为，被告人王登峰身为国家工作人员，利用职务上的便利，侵吞、骗取公共财物，数额特别巨大，其行为构成贪污罪；身为国家工作人员，利用职务上的便利，为他人谋取利益，非法收受他人财物，数额特别巨大，其行为构成受贿罪。王登峰犯数罪，应予并罚。王登峰具有坦白情节，积极退缴全部赃款赃物及孳息，部分贪污犯罪系未遂，具有法定、酌定从轻处罚情节，依法可对其从轻处罚。法院遂依法作出上述判决。王登峰个人履历：1981.09-1985.07北京大学心理学系本科学习，获心理学学士学位；1994.07-1996.12北京大学团委书记（1995.08破格晋升教授）；1996.12-1997.07北京大学党委常委、校团委书记；1999.02-2006.04北京大学党委副书记（2004.02-2006.01挂职江西省吉安市委副书记），北京大学人格与社会心理学研究中心主任（2003.11成立并兼）；2006.04-国家语言文字工作委员会副主任、教育部语言文字应用管理司司长；2011年11月起任教育部体育卫生与艺术教育司司长；2014年1月，当选第十届中国足协副主席2015年起，任全国青少年校园足球工作领导小组办公室主任。2020年，任教育部应对新冠肺炎疫情工作领导小组办公室主任。2021年12月，任全国应用心理专业学位研究生教育指导委员会主任委员2022年6月1日，担任中国青少年足球联赛赛事办公室副主任。 \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n                    关闭\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n亲爱的凤凰网用户:\\n您当前使用的浏览器版本过低，导致网站不能正常访问，建议升级浏览器\\n第三方浏览器推荐:\\n\\n\\n\\n                            谷歌(Chrome)浏览器\\n                                下载\\n                            \\n\\n\\n\\n\\n\\n                            360安全浏览器\\n                                下载\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ', metadata={'source': 'https://news.ifeng.com/c/8Y3TlIcTsj0', 'title': '足协原副主席王登峰被判17年_凤凰网', 'description': '足协原副主席王登峰被判17年', 'language': 'zh'})]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_html"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T09:08:01.211040Z",
     "start_time": "2024-07-01T09:08:01.203346Z"
    }
   },
   "id": "70ef5d24861b2c87",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# persist_directory允许我们将目录保存到磁盘上\n",
    "# 注意from_documents每次运行都是把数据添加进去\n",
    "vectordb = Chroma.from_documents(documents=docs_html, embedding=embedding, persist_directory=\"D:\\workspace\\project\\LLM\\Chroma\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T09:08:10.620390Z",
     "start_time": "2024-07-01T09:08:06.503633Z"
    }
   },
   "id": "3bc7638c0c73df2",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T09:09:24.764840Z",
     "start_time": "2024-07-01T09:09:24.741417Z"
    }
   },
   "id": "e2473203aa174ea1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectordb_load = Chroma(\n",
    "    persist_directory=\"D:\\workspace\\project\\LLM\\Chroma\",\n",
    "    embedding_function=embedding\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T05:37:07.778989Z",
     "start_time": "2024-07-03T05:37:05.786235Z"
    }
   },
   "id": "5c38e1a978368d51",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_load._collection.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T05:37:29.398970Z",
     "start_time": "2024-07-03T05:37:29.389652Z"
    }
   },
   "id": "cc831290054f9141",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Document(page_content='足协原副主席王登峰被判17年_凤凰网\\n\\n\\n\\n首页资讯视频直播凤凰卫视财经娱乐体育时尚汽车房产科技读书文化历史军事旅游佛教更多国学数码健康家居公益教育酒业美食资讯 > 大陆 > 正文足协原副主席王登峰被判17年山东高法2024年03月18日 16:55:00 2024年3月18日上午，山东省德州市中级人民法院一审公开宣判教育部体育卫生与艺术教育司原司长王登峰贪污、受贿案，以贪污罪判处其有期徒刑十四年，并处罚金人民币四百万元；以受贿罪判处其有期徒刑十一年，并处罚金人民币一百万元，决定执行有期徒刑十七年，并处罚金人民币五百万元。查扣在案的贪污赃款依法返还被害单位，受贿犯罪违法所得及孳息依法没收，上缴国库。经审理查明：一、贪污罪。2016年6月至2022年8月，被告人王登峰利用担任教育部体育卫生与艺术教育司司长等的职务便利，单独或伙同他人，采取侵吞、骗取等手段，非法占有公共财物共计4670.13万余元，其中180.49万余元未遂。二、受贿罪。2006年1月至2022年春节前，被告人王登峰利用担任教育部体育卫生与艺术教育司司长等职务便利，为他人在项目承揽、赛事承办等方面谋取利益，单独或伙同他人收受相关单位或个人给予的财物，共计折合人民币966.469万元。德州市中级人民法院认为，被告人王登峰身为国家工作人员，利用职务上的便利，侵吞、骗取公共财物，数额特别巨大，其行为构成贪污罪；身为国家工作人员，利用职务上的便利，为他人谋取利益，非法收受他人财物，数额特别巨大，其行为构成受贿罪。王登峰犯数罪，应予并罚。王登峰具有坦白情节，积极退缴全部赃款赃物及孳息，部分贪污犯罪系未遂，具有法定、酌定从轻处罚情节，依法可对其从轻处罚。法院遂依法作出上述判决。王登峰个人履历：1981.09-1985.07北京大学心理学系本科学习，获心理学学士学位；1994.07-1996.12北京大学团委书记（1995.08破格晋升教授）；1996.12-1997.07北京大学党委常委、校团委书记；1999.02-2006.04北京大学党委副书记（2004.02-2006.01挂职江西省吉安市委副书记），北京大学人格与社会心理学研究中心主任（2003.11成立并兼）；2006.04-国家语言文字工作委员会副主任、教育部语言文字应用管理司司长；2011年11月起任教育部体育卫生与艺术教育司司长；2014年1月，当选第十届中国足协副主席2015年起，任全国青少年校园足球工作领导小组办公室主任。2020年，任教育部应对新冠肺炎疫情工作领导小组办公室主任。2021年12月，任全国应用心理专业学位研究生教育指导委员会主任委员2022年6月1日，担任中国青少年足球联赛赛事办公室副主任。 \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n                    关闭\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n亲爱的凤凰网用户:\\n您当前使用的浏览器版本过低，导致网站不能正常访问，建议升级浏览器\\n第三方浏览器推荐:\\n\\n\\n\\n                            谷歌(Chrome)浏览器\\n                                下载\\n                            \\n\\n\\n\\n\\n\\n                            360安全浏览器\\n                                下载\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ', metadata={'description': '足协原副主席王登峰被判17年', 'language': 'zh', 'source': 'https://news.ifeng.com/c/8Y3TlIcTsj0', 'title': '足协原副主席王登峰被判17年_凤凰网'})]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_load.similarity_search(\"足协\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T05:37:36.018275Z",
     "start_time": "2024-07-03T05:37:34.308195Z"
    }
   },
   "id": "452767ae703a35f4",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 检索式问答链"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef034d00470893a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# 创建提示词模板\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"使用下面的语料来回答本模板最末尾的问题。如果你不知道问题的答案，直接回答 \"我不知道\"，禁止随意编造答案。\n",
    "        为了保证答案尽可能简洁，你的回答必须不超过三句话，你的回答中不可以带有星号。\n",
    "        请注意！在每次回答结束之后，你都必须接上 \"感谢你的提问\" 作为结束语。\n",
    "        请注意！回答语言与提问语言保持一致。\n",
    "        以下是一对问题和答案的样例：\n",
    "            请问：秦始皇的原名是什么\n",
    "            秦始皇原名嬴政。感谢你的提问。\n",
    "        \n",
    "        以下是语料：\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "#创建检索链\n",
    "llm = Tongyi(temperature=0.1)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever = vectordb_load.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T06:54:49.440596Z",
     "start_time": "2024-07-03T06:54:48.479688Z"
    }
   },
   "id": "f6e3a50d7dcd6c0a",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "'王登峰曾任教育部体育卫生与艺术教育司司长、国家语言文字工作委员会副主任、足协原副主席等职务。感谢你的提问。'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"王登峰是谁?\"\n",
    "})\n",
    "response[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T06:56:35.363813Z",
     "start_time": "2024-07-03T06:56:30.417659Z"
    }
   },
   "id": "5ab2475b5d0e75e0",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'王登峰，男，汉族，1963年10月生，山西孝义人，1984年12月加入中国共产党，1987年7月参加工作，北京大学哲学系哲学专业毕业，研究生学历，教育学博士学位，教授。\\n\\n现任教育部体育卫生与艺术教育司司长，中国大学生体育协会主席，中国中学生体育协会主席。'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较：原生模型调用与retrieval_chain调用的区别：\n",
    "llm.invoke(\"王登峰是谁?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T06:58:53.115305Z",
     "start_time": "2024-07-03T06:58:48.845334Z"
    }
   },
   "id": "ad9ffa8a679020fd",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 检索式对话模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd1bc05154c7527"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationSummaryMemory\nllm\n  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmemory\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConversationBufferMemory, ConversationSummaryMemory\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConversationalRetrievalChain\n\u001B[1;32m----> 4\u001B[0m memory \u001B[38;5;241m=\u001B[39m ConversationSummaryMemory(\n\u001B[0;32m      5\u001B[0m     llm\u001B[38;5;241m=\u001B[39mretrieval_chain, memory_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchat_history\u001B[39m\u001B[38;5;124m\"\u001B[39m, return_messages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m qa \u001B[38;5;241m=\u001B[39m ConversationalRetrievalChain\u001B[38;5;241m.\u001B[39mfrom_llm(llm\u001B[38;5;241m=\u001B[39mretrieval_chain, retriever\u001B[38;5;241m=\u001B[39mretriever, memory\u001B[38;5;241m=\u001B[39mmemory)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mValidationError\u001B[0m: 1 validation error for ConversationSummaryMemory\nllm\n  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages (type=type_error)"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T07:22:39.915246Z",
     "start_time": "2024-07-03T07:22:39.849740Z"
    }
   },
   "id": "3a8ab68f299811d9",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "' Wang Dengfeng is a former vice chairman of the Chinese Football Association and former head of the Department of Physical Education, Health, and Arts Education at the Ministry of Education in China. He has been sentenced to 17 years in prison for corruption charges, including贪污 and受贿.'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = qa.invoke(\n",
    "    {\"question\": \"王登峰是谁?\"}\n",
    ")\n",
    "res[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T07:23:49.427562Z",
     "start_time": "2024-07-03T07:23:37.430016Z"
    }
   },
   "id": "84fbfaad5bf40420",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "\" Wang Dengfeng, former vice chairman of the Football Association, was sentenced to 17 years in prison by the Dezhou Intermediate People's Court in Shandong Province for贪污 and受贿 crimes.\""
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = qa.invoke(\n",
    "    {\"question\": \"王登峰怎么样了?\"}\n",
    ")\n",
    "res[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T07:07:10.500071Z",
     "start_time": "2024-07-03T07:07:00.147226Z"
    }
   },
   "id": "bccf7f2db4748155",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='王登峰是谁?'),\n AIMessage(content=' Wang Dengfeng is a former vice chairman of the Chinese Football Association and former head of the Department of Physical Education, Health, and Arts Education at the Ministry of Education in China. He has been sentenced to 17 years in prison for corruption charges, including贪污 and受贿.')]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T07:27:27.163714Z",
     "start_time": "2024-07-03T07:27:27.150472Z"
    }
   },
   "id": "1c5ed3cf0c2c15eb",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "memory.chat_memory.clear()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T07:22:01.607304Z",
     "start_time": "2024-07-03T07:22:01.600556Z"
    }
   },
   "id": "d006598c3608734c",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ec3dcbe52da5d08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
