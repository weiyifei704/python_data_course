{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Tongyi(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt0 = \"请介绍一下微观经济学的前提基础\"\n",
    "llm.invoke(prompt0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e54a42fbcf54ebc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm.get_num_tokens(\"fasfdsafreqwrwe\")  # 需要依赖torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc53418349c56711",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "help(llm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78d985880d357163",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dir(llm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f42e3d44d44051",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_di = Tongyi(temperature=0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e280d871f9122a3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "s = \"请帮我写一副关于龙年的对联\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60f0a629ef82238f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm.invoke(s)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3e248f2e95e51ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_di.invoke(s)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a7ce50e81351b6b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_ga = Tongyi(temperature=0.9)\n",
    "res = llm_ga.generate([\"写一首唐诗绝句\", \"写一首宋词《贺新郎》\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2303852b2fca28f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(res.generations[0][0].text)\n",
    "res.generations[0][0].generation_info"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6f59afb6ad0aeb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(res.generations[1][0].text)\n",
    "res.generations[1][0].generation_info"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfd07f51c8474166",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_ga.invoke(\"这首贺新郎是否符合词牌格律？\")  # 显然模型调用没法识别历史提问上下文"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fa809c58fb23790",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "我希望你能充当新公司的命名顾问。\n",
    "一个生产{product}的公司的好名字是什么？\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt = prompt_template.format(product=\"彩色袜子\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90249a86c4a086b8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(llm_ga.invoke(prompt))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "226ba386ed6086f3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"],\n",
    "    template=\"给我讲一个{adjective}的关于{content}的笑话。\"\n",
    ")\n",
    "prompt = multiple_input_prompt.format(adjective=\"有趣的\", content=\"小鸡\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b843becc427c1c2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(llm_ga.invoke(prompt))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e32e3e4d1e9aa24b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# 首先，创建Few Shot示例列表\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "# 接下来，我们指定用于格式化示例的模板。\n",
    "# 我们使用`PromptTemplate`类来实现这个目的。\n",
    "example_formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "# 最后，创建`FewShotPromptTemplate`对象。\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # 这些是我们要插入到提示中的示例。\n",
    "    examples=examples,\n",
    "    # 这是我们在将示例插入到提示中时要使用的格式。\n",
    "    example_prompt=example_prompt_template,\n",
    "    # 前缀是出现在提示中示例之前的一些文本。\n",
    "    # 通常，这包括一些说明。\n",
    "    prefix=\"Give the antonym of every input\\n\",\n",
    "    # 后缀是出现在提示中示例之后的一些文本。\n",
    "    # 通常，这是用户输入的地方。\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    # 输入变量是整个提示期望的变量。\n",
    "    input_variables=[\"input\"],\n",
    "    # 示例分隔符是我们将前缀、示例和后缀连接在一起的字符串。\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "# 现在，我们可以使用`format`方法生成一个提示。\n",
    "prompt = few_shot_prompt.format(input=\"big\")\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9754cf082b1f449e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm.invoke(few_shot_prompt.format(input=\"生机\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "577b6404ecef6bad",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_ty = ChatTongyi(\n",
    "    streaming=True,\n",
    ")\n",
    "res = chat_ty.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "try:\n",
    "    for chunk in res:\n",
    "        print(chunk)\n",
    "except TypeError as e:\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b1ce221c163fe4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res = chat_ty.stream([HumanMessage(content=\"帮我写副对联\")], streaming=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f51809792fad0f2f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "try:\n",
    "    for chunk in res:\n",
    "        print(chunk.content)\n",
    "except TypeError as e:\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2946f5efa3c7a734",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"我希望你能充当新公司的命名顾问。一个生产{product}的公司好的中文名字是什么,请给出五个选项？\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "747ad659792b5b23",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99ca3c8cecede04b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain.invoke(\"彩色袜子\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20b119ae34cad2c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_list = [\n",
    "    {\"product\": \"袜子\"},\n",
    "    {\"product\": \"电脑\"},\n",
    "    {\"product\": \"男鞋\"}\n",
    "]\n",
    "chain.invoke(input_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e506807b2bfd4268",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for input in input_list:\n",
    "    chain.invoke(input)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dcb063fe5bc4eb2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"请列出五个 {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b350ad3994019d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input0 = prompt.format(subject=\"中国的名山\")\n",
    "output = llm.invoke(input0)\n",
    "res0 = output_parser.parse(output)\n",
    "res0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "674632333742348e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 其他解析器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15d73bb712ab4e34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser, StructuredOutputParser\n",
    "# 日期时间解析器:DatetimeOutputParser\n",
    "# 结构化输出解析器:StructuredOutputParser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ccedd683be54725"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 历史消息"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29f0fae6eab2ec37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory\n",
    "\n",
    "大多数的 LLM 应用程序都会有一个会话接口，允许我们和 LLM 进行多轮的对话，并有一定的上下文记忆能力。但实际上，模型本身是不会记忆任何上下文的，只能依靠用户本身的输入去产生输出。而实现这个记忆功能，就需要额外的模块去保存我们和模型对话的上下文信息，然后在下一次请求时，把所有的历史信息都输入给模型，让模型输出最终结果。\n",
    "\n",
    "而在 LangChain 中，提供这个功能的模块就称为 Memory，用于存储用户和模型交互的历史信息。在 LangChain 中根据功能和返回值的不同，会有多种不同的 Memory 类型，主要可以分为以下几个类别：\n",
    "\n",
    "* 对话缓冲区内存（ConversationBufferMemory），最基础的内存模块，用于存储历史的信息\n",
    "* 对话缓冲器窗口内存（ConversationBufferWindowMemory），只保存最后的 K 轮对话的信息，因此这种内存空间使用会相对较少\n",
    "* 对话摘要内存（ConversationSummaryMemory），这种模式会对历史的所有信息进行抽取，生成摘要信息，然后将摘要信息作为历史信息进行保存。\n",
    "* 对话摘要缓存内存（ConversationSummaryBufferMemory），这个和上面的作用基本一致，但是有最大 token 数的限制，达到这个最大 token 数的时候就会进行合并历史信息生成摘要\n",
    "\n",
    "值得注意的是，对话摘要内存的设计出发点就是语言模型能支持的上下文长度是有限的（一般是 2048），超过了这个长度的数据天然的就被截断了。这个类会根据对话的轮次进行合并，默认值是 2，也就是每 2 轮就开启一次调用 LLM 去合并历史信息。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c0c80c07043dc2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 链外管理内存\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.messages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3061ed7689a2b744",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 对话缓冲区内存（ConversationBufferMemory）\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation.predict(input=\"Hi there!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ef8f8cd5b216e8a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "conversation.predict(input=\"你是谁\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e77f0e9ba6535c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "conversation.memory.chat_memory.messages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea6fdf4f485e68f9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "import json\n",
    "\n",
    "dicts = messages_to_dict(conversation.memory.chat_memory.messages)\n",
    "dicts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42f5b2a774eee838",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"./memory\", 'wb')\n",
    "pickle.dump(dicts, f)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa74b30865720cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dicts_load = pickle.load(open(\"./memory\", \"rb\"))\n",
    "dicts_load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9f7ae23b92ba10b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 向对话添加记忆\n",
    "new_messages = messages_from_dict(dicts_load)\n",
    "retrieved_chat_history = ChatMessageHistory(messages=new_messages)\n",
    "retrieved_memory = ConversationBufferMemory(chat_memory=retrieved_chat_history)\n",
    "\n",
    "conversation_reload = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=retrieved_memory\n",
    ")\n",
    "conversation_reload.predict(input=\"我回来了\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b976c1c1823dae6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2c793c5c5d00e80d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 检索增强 与 文本向量器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38165cbdc18f6830"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 文档导入"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53a9c20e5c2a7a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "model_ty = Tongyi(temperature=0.1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd945edcf2e15761",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 不用print函数，直接用变量名称打印输出\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-05T04:46:34.272966Z",
     "start_time": "2024-07-05T04:46:34.249090Z"
    }
   },
   "id": "f6d20d7d0600847e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader_pdf = PyPDFLoader(r\"D:\\workspace\\收发文件\\新型冠状病毒肺炎防控方案（第八版）.pdf\")\n",
    "docs_pdf = loader_pdf.load()\n",
    "type(docs_pdf)\n",
    "len(docs_pdf)\n",
    "type(docs_pdf[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c5b1337e26157b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "docs_pdf[0].metadata\n",
    "docs_pdf[0].page_content[0:100]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76e58d43ead0b0ba",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader_txt = TextLoader(r'D:\\workspace\\收发文件\\李卅移交资料.txt', encoding='utf8')\n",
    "docs_txt = loader_txt.load()\n",
    "len(docs_txt)\n",
    "\n",
    "docs_txt[0].metadata\n",
    "docs_txt[0].page_content[0:100]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a3dfc7eca2f5aae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "WEB_URL = \"https://news.ifeng.com/c/8Y3TlIcTsj0\"\n",
    "loader_html = WebBaseLoader(WEB_URL)\n",
    "docs_html = loader_html.load()\n",
    "\n",
    "len(docs_html)\n",
    "docs_html[0].metadata\n",
    "docs_html[0].page_content[:100]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a9903317162d847",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 导入递归字符文本分割器\n",
    "text_splitter_txt = RecursiveCharacterTextSplitter(chunk_size=384, chunk_overlap=0,\n",
    "                                                   separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \"。\", \"，\"])\n",
    "# 导入文本\n",
    "documents_txt = text_splitter_txt.split_documents(docs_txt)\n",
    "len(documents_txt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f0e69946c83af57",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "documents_txt[0].page_content"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc22cbea0d584dd3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "___"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "853ba8e89f3bca7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_transformer = SentenceTransformer('D:\\workspace\\project\\LLM\\models\\m3e-base')\n",
    "sentences = [\n",
    "    '* Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem',\n",
    "    '* Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练',\n",
    "    '* Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one'\n",
    "]\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb = sentence_transformer.encode(sentences)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "961c7c6f8c4f0530",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query1 = \"狗\"\n",
    "query2 = \"猫\"\n",
    "query3 = \"雨\"\n",
    "# 通过对应的 embedding 类生成 query 的 embedding。\n",
    "emb1 = sentence_transformer.encode(query1)\n",
    "emb2 = sentence_transformer.encode(query2)\n",
    "emb3 = sentence_transformer.encode(query3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e95fca3f90f541e1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 将返回结果转成 numpy 的格式，便于后续计算\n",
    "emb1 = np.array(emb1)\n",
    "emb2 = np.array(emb2)\n",
    "emb3 = np.array(emb3)\n",
    "\n",
    "np.dot(emb1, emb2)\n",
    "np.dot(emb3, emb2)\n",
    "np.dot(emb1, emb3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8354e57d0a3fc5a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# sentenceTransformer = SentenceTransformer('D:\\workspace\\project\\LLM\\models\\m3e-base')\n",
    "embedding = SentenceTransformerEmbeddings(model_name='D:\\workspace\\project\\LLM\\models\\m3e-base')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc84dfacbe2776f8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "WEB_URL = \"https://news.ifeng.com/c/8Y3TlIcTsj0\"\n",
    "loader_html = WebBaseLoader(WEB_URL)\n",
    "docs_html = loader_html.load()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ea522758f17b518",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "docs_html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70ef5d24861b2c87",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# persist_directory允许我们将目录保存到磁盘上\n",
    "# 注意from_documents每次运行都是把数据添加进去\n",
    "vectordb = Chroma.from_documents(documents=docs_html, embedding=embedding,\n",
    "                                 persist_directory=\"D:\\workspace\\project\\LLM\\Chroma\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bc7638c0c73df2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2473203aa174ea1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectordb_load = Chroma(\n",
    "    persist_directory=\"D:\\workspace\\project\\LLM\\Chroma\",\n",
    "    embedding_function=embedding\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c38e1a978368d51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectordb_load._collection.count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc831290054f9141",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectordb_load.similarity_search(\"足协\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "452767ae703a35f4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 检索式问答链"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef034d00470893a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# 创建提示词模板\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"使用下面的语料来回答本模板最末尾的问题。如果你不知道问题的答案，直接回答 \"我不知道\"，禁止随意编造答案。\n",
    "        为了保证答案尽可能简洁，你的回答必须不超过三句话，你的回答中不可以带有星号。\n",
    "        请注意！在每次回答结束之后，你都必须接上 \"感谢你的提问\" 作为结束语。\n",
    "        请注意！回答语言与提问语言保持一致。\n",
    "        以下是一对问题和答案的样例：\n",
    "            请问：秦始皇的原名是什么\n",
    "            秦始皇原名嬴政。感谢你的提问。\n",
    "        \n",
    "        以下是语料：\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "#创建检索链\n",
    "llm = Tongyi(temperature=0.1)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever = vectordb_load.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6e3a50d7dcd6c0a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"王登峰是谁?\"\n",
    "})\n",
    "response[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ab2475b5d0e75e0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 比较：原生模型调用与retrieval_chain调用的区别：\n",
    "llm.invoke(\"王登峰是谁?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9ffa8a679020fd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 检索式对话模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd1bc05154c7527"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "D:\\Program\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "llm = Tongyi()\n",
    "\n",
    "embedding = SentenceTransformerEmbeddings(model_name='D:\\workspace\\project\\LLM\\models\\m3e-base')\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"D:\\workspace\\project\\LLM\\Chroma\",\n",
    "    embedding_function=embedding\n",
    ")\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T04:47:40.126082Z",
     "start_time": "2024-07-08T04:47:30.165093Z"
    }
   },
   "id": "3a8ab68f299811d9",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "' 王登峰是教育部体育卫生与艺术教育司的原司长，也曾担任足协原副主席。'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"请用中文回答：{question}\"\n",
    "prompt = PromptTemplate(input_variables=[\"question\"],\n",
    "                        template=template, )\n",
    "\n",
    "res = qa.invoke(\n",
    "    {\"question\": prompt.format(question=\"王登峰是谁?\")}\n",
    ")\n",
    "res[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T04:48:08.099332Z",
     "start_time": "2024-07-08T04:47:59.853753Z"
    }
   },
   "id": "84fbfaad5bf40420",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "        {\"question\": RunnablePassthrough()}\n",
    "        |{\"question\": prompt}\n",
    "        |qa\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T04:50:44.923392Z",
     "start_time": "2024-07-08T04:50:44.918515Z"
    }
   },
   "id": "d006598c3608734c",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for HumanMessage\ncontent\n  str type expected (type=type_error.str)\ncontent\n  value is not a valid list (type=type_error.list)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m chain\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m王登峰是谁?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2507\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   2505\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2506\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2507\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n\u001B[0;32m   2508\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   2509\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    167\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:161\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    156\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    159\u001B[0m     )\n\u001B[1;32m--> 161\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    162\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[0;32m    163\u001B[0m     )\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:460\u001B[0m, in \u001B[0;36mChain.prep_outputs\u001B[1;34m(self, inputs, outputs, return_only_outputs)\u001B[0m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_outputs(outputs)\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39msave_context(inputs, outputs)\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_only_outputs:\n\u001B[0;32m    462\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain\\memory\\summary.py:90\u001B[0m, in \u001B[0;36mConversationSummaryMemory.save_context\u001B[1;34m(self, inputs, outputs)\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_context\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any], outputs: Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     89\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 90\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msave_context(inputs, outputs)\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict_new_summary(\n\u001B[0;32m     92\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_memory\u001B[38;5;241m.\u001B[39mmessages[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m:], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer\n\u001B[0;32m     93\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain\\memory\\chat_memory.py:57\u001B[0m, in \u001B[0;36mBaseChatMemory.save_context\u001B[1;34m(self, inputs, outputs)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001B[39;00m\n\u001B[0;32m     55\u001B[0m input_str, output_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_input_output(inputs, outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_memory\u001B[38;5;241m.\u001B[39madd_messages(\n\u001B[1;32m---> 57\u001B[0m     [HumanMessage(content\u001B[38;5;241m=\u001B[39minput_str), AIMessage(content\u001B[38;5;241m=\u001B[39moutput_str)]\n\u001B[0;32m     58\u001B[0m )\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\messages\\human.py:48\u001B[0m, in \u001B[0;36mHumanMessage.__init__\u001B[1;34m(self, content, **kwargs)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28mself\u001B[39m, content: Union[\u001B[38;5;28mstr\u001B[39m, List[Union[\u001B[38;5;28mstr\u001B[39m, Dict]]], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[0;32m     46\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Pass in content as positional arg.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(content\u001B[38;5;241m=\u001B[39mcontent, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\langchain_core\\messages\\base.py:61\u001B[0m, in \u001B[0;36mBaseMessage.__init__\u001B[1;34m(self, content, **kwargs)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m, content: Union[\u001B[38;5;28mstr\u001B[39m, List[Union[\u001B[38;5;28mstr\u001B[39m, Dict]]], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[0;32m     59\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Pass in content as positional arg.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 61\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(content\u001B[38;5;241m=\u001B[39mcontent, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Program\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mValidationError\u001B[0m: 2 validation errors for HumanMessage\ncontent\n  str type expected (type=type_error.str)\ncontent\n  value is not a valid list (type=type_error.list)"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"王登峰是谁?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T04:50:50.954853Z",
     "start_time": "2024-07-08T04:50:46.499230Z"
    }
   },
   "id": "1ec3dcbe52da5d08",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9e3665ab3da3bba2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
